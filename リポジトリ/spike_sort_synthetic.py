# -*- coding: utf-8 -*-
"""
Spike sorting pipeline for synthetic extracellular RAW files.

- Loads interleaved int16 little-endian RAW generated by launch_synthetic_raw.pyw.
- Allows the user to pick the channel count via a small Tkinter GUI when necessary.
- Runs spike detection, waveform extraction, PCA feature reduction, HDBSCAN clustering,
  simple template refinement, and autocorrelogram based quality control.
- Saves intermediate artefacts (figures + numerical data) and writes final results to
  an HDF5 file with per-cluster groups.
"""

from __future__ import annotations

import argparse
import datetime as dt
import functools
import os
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, Sequence
import itertools

import h5py
import matplotlib


matplotlib.use("Agg")  # headless figure generation
import matplotlib.pyplot as plt
import numpy as np
from hdbscan import HDBSCAN
from scipy import signal
from scipy.signal import argrelmin, firwin, lfilter
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

THIS_FILE = Path(__file__).resolve()
SCRIPT_DIR = THIS_FILE.parent
MS = 1000
COLOR = [
    "b",
    "chartreuse",
    "r",
    "c",
    "m",
    "y",
    "k",
    "brown",
    "forestgreen",
    "darkcyan",
    "maroon",
    "orange",
    "green",
    "steelblue",
    "purple",
    "gold",
    "navy",
    "gray",
    "indigo",
    "black",
    "darkgoldenrod",
]


@dataclass(frozen=True)
class SortingConfig:
    fs: int = 20_000
    band_bottom: int = 300
    band_top: int = 3_000
    band_numtaps: int = 255
    spike_threshold_sd: float = 4.0
    spike_order: int = 15
    spike_polarity: int = -1
    window_before_ms: float = 1.0
    window_after_ms: float = 2.0
    cut_area: int = 13
    pca_components: int = 2
    cluster_min_size: int = 2000
    cluster_min_samples: int = 250
    noise_reassign_score: int = 72
    template_merge_score: int = 115


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Spike sorting for synthetic RAW files")
    parser.add_argument("--raw", type=Path, default=None, help="Path to RAW file")
    parser.add_argument("--fs", type=int, default=SortingConfig.fs, help="Sampling rate [Hz]")
    parser.add_argument("--n-channels", type=int, default=None, help="Total number of channels (omit to select via GUI)")
    parser.add_argument(
        "--channels",
        type=int,
        nargs="*",
        default=None,
        help="Subset of channel indices to process (default: all)",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("sorting_results"),
        help="Working directory for figures and HDF5 output",
    )
    parser.add_argument(
        "--duration-hint",
        type=float,
        default=None,
        help="Expected recording duration [s] to guide channel inference",
    )
    parser.add_argument("--no-gui", action="store_true", help="Disable Tk GUI prompts (fails if n-channels missing)")
    return parser.parse_args()


def candidate_channels(total_samples: int, fs: int) -> list[tuple[int, float]]:
    candidates: list[tuple[int, float]] = []
    for n_ch in range(1, min(4096, total_samples) + 1):
        if total_samples % n_ch:
            continue
        samples_per_channel = total_samples // n_ch
        duration = samples_per_channel / fs
        if duration <= 0:
            continue
        candidates.append((n_ch, duration))
    return candidates


def score_candidates(
    candidates: list[tuple[int, float]],
    duration_hint: float | None,
    default_channels: int | None,
) -> list[tuple[int, float, tuple[float, float, float]]]:
    scored: list[tuple[int, float, tuple[float, float, float]]] = []
    for n_ch, duration in candidates:
        duration_penalty = 0.0 if not duration_hint else abs(duration - duration_hint)
        channel_penalty = 0.0 if default_channels is None else abs(n_ch - default_channels) * 0.05
        scored.append((n_ch, duration, (duration_penalty, channel_penalty, -n_ch)))
    scored.sort(key=lambda item: item[2])
    return scored


def prompt_channel_gui(
    scored_candidates: Sequence[tuple[int, float, tuple[float, float, float]]],
    default_value: int | None,
    raw_path: Path,
    fs: int,
) -> int | None:
    """Show a Tk dialog to choose channel count."""
    try:
        import tkinter as tk
        from tkinter import ttk, messagebox
    except Exception:
        return None

    options = list(scored_candidates[:12])
    root = tk.Tk()
    root.title('Select channel count')
    root.resizable(False, False)

    description = (
        f'File: {raw_path.name}\n'
        f'Sampling rate: {fs} Hz\n'
        'Choose a channel count or enter a value manually.'
    )
    ttk.Label(root, text=description, justify=tk.LEFT, padding=10).grid(
        row=0, column=0, columnspan=2, sticky=tk.W
    )

    listbox = tk.Listbox(root, height=len(options), width=36, exportselection=False)
    for idx, (n_ch, duration, _) in enumerate(options, start=1):
        label = f"{n_ch} ch (~ {duration:.2f} s)"
        listbox.insert(tk.END, label)
        if default_value and n_ch == default_value:
            listbox.selection_set(idx - 1)
    if not listbox.curselection() and options:
        listbox.selection_set(0)
    listbox.grid(row=1, column=0, columnspan=2, padx=10, pady=4, sticky=tk.EW)

    ttk.Label(root, text='Manual override (int)').grid(
        row=2, column=0, padx=10, pady=(8, 2), sticky=tk.W
    )
    manual_var = tk.StringVar()
    ttk.Entry(root, textvariable=manual_var, width=12).grid(
        row=2, column=1, padx=10, pady=(8, 2), sticky=tk.W
    )

    result: dict[str, int | None] = {'value': None}

    def on_ok() -> None:
        selection = listbox.curselection()
        if selection:
            result['value'] = options[selection[0]][0]
        manual = manual_var.get().strip()
        if manual:
            try:
                manual_value = int(manual)
            except ValueError:
                messagebox.showerror('Invalid input', 'Enter an integer channel count.')
                return
            if manual_value <= 0:
                messagebox.showerror('Invalid input', 'Channel count must be positive.')
                return
            result['value'] = manual_value
        if result['value'] is None:
            messagebox.showinfo('Selection required', 'Choose or enter a channel count.')
            return
        root.destroy()

    def on_cancel() -> None:
        result['value'] = None
        root.destroy()

    btn_frame = ttk.Frame(root)
    btn_frame.grid(row=3, column=0, columnspan=2, pady=10)
    ttk.Button(btn_frame, text='OK', command=on_ok, width=12).pack(side=tk.LEFT, padx=6)
    ttk.Button(btn_frame, text='Cancel', command=on_cancel, width=12).pack(side=tk.LEFT, padx=6)

    root.protocol('WM_DELETE_WINDOW', on_cancel)
    root.mainloop()
    return result['value']

def _ask_raw_path(initial: Path | None = None) -> Path:
    try:
        import tkinter as tk
        from tkinter import filedialog
    except Exception as exc:
        raise RuntimeError(
            "Tkinter is required to select a RAW file. Please pass --raw instead."
        ) from exc

    root = tk.Tk()
    root.withdraw()
    selected = filedialog.askopenfilename(
        title="Select RAW file",
        filetypes=[("RAW files", "*.raw"), ("All files", "*.*")],
        initialdir=str((initial or SCRIPT_DIR).resolve()),
    )
    root.destroy()
    if not selected:
        raise RuntimeError("No RAW file selected; aborting.")
    return Path(selected)

def infer_channel_count(
    raw_path: Path,
    fs: int,
    n_channels_arg: int | None,
    duration_hint: float | None,
    allow_gui: bool,
) -> int:
    if n_channels_arg is not None:
        if n_channels_arg <= 0:
            raise ValueError("n_channels must be positive")
        return n_channels_arg

    total_samples = raw_path.stat().st_size // np.dtype("<i2").itemsize
    candidates = candidate_channels(total_samples, fs)
    if not candidates:
        raise RuntimeError("RAW ファイルサイズからチャンネル数を推定できませんでした。")

    default_channels = None
    scored = score_candidates(candidates, duration_hint, default_channels)
    if allow_gui:
        selection = prompt_channel_gui(scored, default_channels, raw_path, fs)
        if selection:
            return selection
    # fallback to best score
    return scored[0][0]


def load_channel_waveform(raw_path: Path, n_channels: int, channel_index: int) -> np.ndarray:
    data = np.fromfile(raw_path, dtype="<i2")
    reshaped = data.reshape(-1, n_channels)
    return reshaped[:, channel_index].astype(np.float32)


def bandpass_filter(wave: np.ndarray, cfg: SortingConfig) -> np.ndarray:
    nyq = cfg.fs / 2
    cutoff = np.array([cfg.band_bottom, cfg.band_top]) / nyq
    taps = firwin(cfg.band_numtaps, cutoff, pass_zero=False)
    filtered = lfilter(taps, 1, wave)
    delay = (cfg.band_numtaps - 1) // 2
    return filtered[delay:]


def detect_spikes(filtered: np.ndarray, cfg: SortingConfig) -> np.ndarray:
    peaks = argrelmin(-cfg.spike_polarity * filtered, order=cfg.spike_order)[0]
    if peaks.size == 0:
        return peaks
    median = np.median(filtered)
    mad = np.median(np.abs(filtered - median)) / 0.6745
    if mad == 0:
        mad = np.std(filtered) or 1.0
    if cfg.spike_polarity < 0:
        threshold = median - cfg.spike_threshold_sd * mad
        mask = filtered[peaks] < threshold
    else:
        threshold = median + cfg.spike_threshold_sd * mad
        mask = filtered[peaks] > threshold
    return peaks[mask]


def collect_waveforms(
    filtered: np.ndarray,
    spike_index: np.ndarray,
    cfg: SortingConfig,
) -> tuple[np.ndarray, np.ndarray]:
    if spike_index.size == 0:
        return np.empty((0, 0), dtype=np.float32), spike_index
    before = int(cfg.window_before_ms * cfg.fs / MS)
    after = int(cfg.window_after_ms * cfg.fs / MS)
    indices = np.c_[spike_index - before, spike_index + after]
    segments = [np.arange(start, stop) for start, stop in indices]
    segment_array = np.array(segments)
    invalid_tail = np.where(segment_array[:, -1] >= filtered.size - 1)[0]
    invalid_head = np.where(segment_array[:, 0] < 0)[0]
    valid = np.ones(segment_array.shape[0], dtype=bool)
    valid[invalid_tail] = False
    valid[invalid_head] = False
    segment_array = segment_array[valid]
    cleaned_indices = spike_index[valid]
    waveforms = filtered[segment_array]
    return waveforms.astype(np.float32), cleaned_indices


def cut_waveforms(spike_shape: np.ndarray, cfg: SortingConfig) -> np.ndarray:
    if spike_shape.size == 0:
        return spike_shape
    roi = np.arange(
        spike_shape.shape[1] / 3 - cfg.cut_area,
        spike_shape.shape[1] / 3 + 2 * cfg.cut_area + 1,
    ).astype(int)
    return spike_shape[:, roi]


def pca_features(waveforms: np.ndarray, cfg: SortingConfig) -> tuple[np.ndarray, np.ndarray]:
    if waveforms.size == 0:
        return np.empty((0, cfg.pca_components)), np.empty(0)
    combo = np.c_[np.diff(waveforms, n=1), np.diff(waveforms, n=2)]
    pca = PCA(n_components=cfg.pca_components)
    transformed = pca.fit_transform(combo)
    return transformed, pca.explained_variance_ratio_


def cluster_spikes(features: np.ndarray, cfg: SortingConfig) -> np.ndarray:
    if features.shape[0] < 5:
        return np.full(features.shape[0], -1, dtype=int)
    scaler = StandardScaler()
    feat_scaled = scaler.fit_transform(features)
    try:
        clusters = HDBSCAN(
            min_cluster_size=cfg.cluster_min_size,
            min_samples=cfg.cluster_min_samples,
            leaf_size=100,
            cluster_selection_method="leaf",
            allow_single_cluster=True,
        ).fit_predict(feat_scaled)
        if np.unique(clusters).size == 1:
            raise ValueError
        return clusters
    except ValueError:
        try:
            clusters = HDBSCAN(
                min_cluster_size=320,
                min_samples=10,
                leaf_size=100,
                cluster_selection_method="leaf",
                allow_single_cluster=True,
            ).fit_predict(feat_scaled)
            if np.unique(clusters).size == 1:
                raise ValueError
            return clusters
        except ValueError:
            hdb = HDBSCAN(min_cluster_size=10, min_samples=100, allow_single_cluster=True)
            hdb.fit(feat_scaled)
            clusters = hdb.labels_
            clusters[hdb.probabilities_ < 0.3] = -1
            return clusters
        except Exception:
            return np.full(features.shape[0], -1, dtype=int)
    except Exception:
        return np.full(features.shape[0], -1, dtype=int)

def make_templates(labels: np.ndarray, wave_shape: np.ndarray) -> dict[int, tuple[np.ndarray, np.ndarray]]:
    templates: dict[int, tuple[np.ndarray, np.ndarray]] = {}
    for clu in np.unique(labels):
        if clu < 0:
            continue
        waves = wave_shape[labels == clu]
        templates[clu] = (waves.mean(axis=0), waves.std(axis=0))
    return templates


def check_template(template: tuple[np.ndarray, np.ndarray], wave: np.ndarray) -> tuple[int, int]:
    temp_mean, temp_sd = template
    lower = wave > (temp_mean - temp_sd)
    upper = wave < (temp_mean + temp_sd)
    combined = lower.astype(int) + upper.astype(int)
    band_score = int(combined[11:14].sum())
    total_score = int(combined.sum())
    return total_score, band_score


def merge_clusters(labels: np.ndarray, wave_shape: np.ndarray, cfg: SortingConfig) -> np.ndarray:
    new_labels = labels.copy()
    templates = make_templates(new_labels, wave_shape)
    clusters = sorted([c for c in templates.keys()])
    for i, clu_i in enumerate(clusters):
        for clu_j in clusters[i + 1 :]:
            total, band = check_template(templates[clu_i], templates[clu_j][0])
            if total >= cfg.template_merge_score and band == 6:
                new_labels[new_labels == clu_j] = clu_i
    return new_labels


def rescue_noise(labels: np.ndarray, wave_shape: np.ndarray, cfg: SortingConfig) -> np.ndarray:
    new_labels = labels.copy()
    noise_idx = np.where(labels == -1)[0]
    if noise_idx.size == 0:
        return new_labels
    templates = make_templates(new_labels, wave_shape)
    if not templates:
        return new_labels
    for idx in noise_idx:
        wave = wave_shape[idx]
        for clu, template in templates.items():
            total, band = check_template(template, wave)
            if total >= cfg.noise_reassign_score and band >= 6:
                new_labels[idx] = clu
                break
    return new_labels


def autocorrelogram(spike_times_ms: np.ndarray, window_ms: int = 1000, bin_ms: int = 1) -> np.ndarray:
    bins = int((window_ms * 2) / bin_ms) + 1
    hist = np.zeros(bins, dtype=float)
    for t in spike_times_ms:
        left = t - window_ms
        right = t + window_ms
        idx = np.where((spike_times_ms >= left) & (spike_times_ms <= right))[0]
        rel = spike_times_ms[idx] - t
        hist += np.histogram(rel, bins=bins, range=(-window_ms, window_ms))[0]
    return hist


def fire_index(acr: np.ndarray, bin_ms: int = 1) -> float:
    x_axis = np.arange(-(acr.size // 2), acr.size // 2 + 1) * bin_ms
    mask = (x_axis >= -200) & (x_axis <= 200)
    core = acr[mask]
    centre_idx = np.array([-2, -1, 1, 2]) + acr.size // 2
    centre = acr[centre_idx]
    if core.sum() == 0:
        return np.inf
    return float(centre.sum() / core.sum() * 100)


def power_spectrum(acr: np.ndarray, fs: int = 1000) -> tuple[np.ndarray, np.ndarray]:
    freq, power = signal.periodogram(acr, fs=fs)
    return freq, power


def ensure_output_dirs(base_dir: Path) -> dict[str, Path]:
    subdirs = {
        "spike_detect": base_dir / "spike_detect",
        "pca": base_dir / "pca",
        "sorting_cluster": base_dir / "sorting_cluster",
        "auto_correlo": base_dir / "auto_correlo",
    }
    for path in subdirs.values():
        path.mkdir(parents=True, exist_ok=True)
    return subdirs


def save_detection_plot(path: Path, filtered: np.ndarray, spike_idx: np.ndarray, fs: int) -> None:
    duration = filtered.size / fs
    time = np.arange(filtered.size) / fs
    fig, ax = plt.subplots(figsize=(10, 3))
    ax.plot(time, filtered, lw=0.5, color="steelblue")
    if spike_idx.size:
        ax.plot(spike_idx / fs, filtered[spike_idx], "r.", ms=3)
    ax.set_xlabel("Time [s]")
    ax.set_ylabel("Filtered (μV)")
    ax.set_title(f"Spike detection (duration {duration:.2f}s)")
    fig.tight_layout()
    fig.savefig(path, dpi=200)
    plt.close(fig)


def save_pca_plots(base_path: Path, features: np.ndarray, labels: np.ndarray, channel_label: str) -> None:
    if features.size == 0:
        return
    fig, ax = plt.subplots(figsize=(5, 4))
    ax.scatter(features[:, 0], features[:, 1], s=5, c="gray", alpha=0.5)
    ax.set_xlabel("PC1")
    ax.set_ylabel("PC2")
    ax.set_title(f"PCA scatter {channel_label}")
    fig.tight_layout()
    fig.savefig(base_path / f"{channel_label}_pca_raw.png", dpi=200)
    plt.close(fig)

    unique_labels = np.unique(labels)
    colors = itertools.cycle(COLOR)
    fig, ax = plt.subplots(figsize=(5, 4))
    for clu, color in zip(unique_labels, colors):
        mask = labels == clu
        ax.scatter(features[mask, 0], features[mask, 1], s=8, label=f"clu {clu}", alpha=0.7, c=color)
    ax.legend(fontsize=8)
    ax.set_xlabel("PC1")
    ax.set_ylabel("PC2")
    ax.set_title(f"PCA clustered {channel_label}")
    fig.tight_layout()
    fig.savefig(base_path / f"{channel_label}_pca_cluster.png", dpi=200)
    plt.close(fig)


def save_cluster_waveforms(base_path: Path, waveforms: np.ndarray, labels: np.ndarray, channel_label: str) -> None:
    unique_labels = np.unique(labels)
    for clu, color in zip(unique_labels, itertools.cycle(COLOR)):
        mask = labels == clu
        if clu < 0 or not mask.any():
            continue
        fig, ax = plt.subplots(figsize=(6, 3))
        ax.plot(waveforms[mask].T, color=color, alpha=0.2, lw=0.5)
        median = np.median(waveforms[mask], axis=0)
        ax.plot(median, color=color, lw=2)
        ax.set_title(f"{channel_label} cluster {clu} (n={mask.sum()})")
        ax.set_xlabel("Samples")
        ax.set_ylabel("μ"μV")
        fig.tight_layout()
        fig.savefig(base_path / f"{channel_label}_cluster{clu}_waveforms.png", dpi=200)
        plt.close(fig)


def save_autocorrelogram(base_path: Path, x_axis: np.ndarray, acr: np.ndarray, channel_label: str, clu: int) -> None:
    fig, ax = plt.subplots(figsize=(5, 3))
    ax.plot(x_axis, acr, color="black", lw=1)
    ax.set_xlim(-200, 200)
    ax.set_xlabel("Time lag [ms]")
    ax.set_ylabel("Autocorrelation")
    ax.set_title(f"{channel_label} cluster {clu} autocorrelogram")
    fig.tight_layout()
    fig.savefig(base_path / f"{channel_label}_cluster{clu}_acr.png", dpi=200)
    plt.close(fig)


def save_power_spectrum(base_path: Path, freq: np.ndarray, power: np.ndarray, channel_label: str, clu: int) -> None:
    fig, ax = plt.subplots(figsize=(5, 3))
    ax.plot(freq, power, color="black", lw=1)
    ax.set_xlim(0, 80)
    ax.set_xlabel("Frequency [Hz]")
    ax.set_ylabel("Power/frequency")
    ax.set_title(f"{channel_label} cluster {clu} power spectrum")
    fig.tight_layout()
    fig.savefig(base_path / f"{channel_label}_cluster{clu}_power.png", dpi=200)
    plt.close(fig)


def main() -> None:
    args = parse_args()
    raw_path = args.raw
    if raw_path is None:
        raw_path = _ask_raw_path()
    raw_path = raw_path.expanduser().resolve()
    if not raw_path.exists():
        raise FileNotFoundError(raw_path)

    cfg = SortingConfig(fs=args.fs)
    n_channels = infer_channel_count(raw_path, cfg.fs, args.n_channels, args.duration_hint, not args.no_gui)

    if args.channels is None or len(args.channels) == 0:
        channels = list(range(n_channels))
    else:
        channels = args.channels
    for ch in channels:
        if ch < 0 or ch >= n_channels:
            raise ValueError(f"チャンネル {ch} は範囲外です (0..{n_channels-1}).")

    timestamp = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    base_dir = (args.output_dir / f"spike_sort_{raw_path.stem}_{timestamp}").resolve()
    base_dir.mkdir(parents=True, exist_ok=True)
    figure_dirs = ensure_output_dirs(base_dir)

    h5_path = base_dir / f"{raw_path.stem}_sorting.h5"
    h5 = h5py.File(h5_path, "w")
    h5.attrs["raw_path"] = str(raw_path)
    h5.attrs["fs"] = cfg.fs
    h5.attrs["n_channels"] = n_channels
    h5.attrs["created_at"] = timestamp

    print(f"RAW: {raw_path}")
    print(f"チャンネル数: {n_channels} (処理対象: {channels})")
    print(f"出力先: {base_dir}")

    for ch in channels:
        channel_label = f"ch{ch:03d}"
        print(f"Processing {channel_label} ...")
        raw_wave = load_channel_waveform(raw_path, n_channels, ch) / 10.0
        filtered = bandpass_filter(raw_wave, cfg)
        spike_idx = detect_spikes(filtered, cfg)
        waveforms, spike_idx = collect_waveforms(filtered, spike_idx, cfg)
        if spike_idx.size == 0:
            print(f"  -> スパイクが検出されませんでした。スキップします。")
            continue
        waveforms_roi = cut_waveforms(waveforms, cfg)
        features, variance = pca_features(waveforms_roi, cfg)
        labels = cluster_spikes(features, cfg)
        labels = merge_clusters(labels, waveforms, cfg)
        labels = rescue_noise(labels, waveforms_roi, cfg)

        spike_times_ms = spike_idx / (cfg.fs / MS)
        # detection figure
        save_detection_plot(
            figure_dirs["spike_detect"] / f"{raw_path.stem}_{channel_label}_spike_detect.png",
            filtered,
            spike_idx,
            cfg.fs,
        )
        save_pca_plots(figure_dirs["pca"], features, labels, f"{raw_path.stem}_{channel_label}")
        save_cluster_waveforms(
            figure_dirs["sorting_cluster"], waveforms, labels, f"{raw_path.stem}_{channel_label}"
        )

        channel_group = h5.create_group(channel_label)
        channel_group.attrs["channel_index"] = ch
        channel_group.create_dataset("spike_indices", data=spike_idx, compression="gzip")
        channel_group.create_dataset("waveforms", data=waveforms, compression="gzip")
        channel_group.create_dataset("waveforms_roi", data=waveforms_roi, compression="gzip")
        channel_group.create_dataset("features", data=features, compression="gzip")
        channel_group.create_dataset("cluster_labels", data=labels, compression="gzip")
        channel_group.create_dataset("spike_times_ms", data=spike_times_ms, compression="gzip")
        channel_group.create_dataset("pca_variance", data=variance)

        clusters_group = channel_group.create_group("clusters")
        x_axis = np.arange(-1000, 1001)
        for clu in sorted(np.unique(labels)):
            mask = labels == clu
            if clu < 0 or not mask.any():
                continue
            spikes_ms = spike_times_ms[mask]
            acr = autocorrelogram(spikes_ms)
            fi = fire_index(acr)
            freq, power = power_spectrum(acr)

            if fi > 1:
                print(f"  -> cluster {clu} FireIndex={fi:.2f} : reject (ラベルを -1 に変更)")
                labels[mask] = -1
                continue

            save_autocorrelogram(
                figure_dirs["auto_correlo"],
                x_axis,
                acr,
                f"{raw_path.stem}_{channel_label}",
                clu,
            )
            save_power_spectrum(
                figure_dirs["auto_correlo"],
                freq,
                power,
                f"{raw_path.stem}_{channel_label}",
                clu,
            )

            cluster_group = clusters_group.create_group(f"cluster_{clu}")
            cluster_group.attrs["fire_index"] = fi
            cluster_group.create_dataset("indices", data=np.where(mask)[0])
            cluster_group.create_dataset("spike_times_ms", data=spikes_ms, compression="gzip")
            cluster_group.create_dataset("waveforms", data=waveforms[mask], compression="gzip")
            cluster_group.create_dataset("waveforms_roi", data=waveforms_roi[mask], compression="gzip")
            cluster_group.create_dataset("features", data=features[mask], compression="gzip")
            cluster_group.create_dataset(
                "autocorrelogram",
                data=np.c_[x_axis, acr],
                compression="gzip",
            )
            cluster_group.create_dataset("power_spectrum", data=np.c_[freq, power], compression="gzip")

        # save updated labels after rejection
        channel_group["cluster_labels"][...] = labels

    h5.close()
    print(f"HDF5 results saved to {h5_path}")


if __name__ == "__main__":
    main()









