{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1c9a2b7-9891-4931-9963-6c7fc818c1ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T07:31:38.643126Z",
     "start_time": "2025-09-12T07:31:36.190952Z"
    }
   },
   "outputs": [],
   "source": [
    "#　解析に使用する定数の定義\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from datetime import datetime\n",
    "from scipy.signal import firwin, lfilter\n",
    "from scipy.signal import argrelmin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hdbscan import HDBSCAN\n",
    "from scipy import signal\n",
    "import os\n",
    "import itertools\n",
    "import functools\n",
    "import glob\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f2a1e6-4c5b-4280-b784-8e6f561292c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw dataからtrigger情報を取得\n",
    "#import numpy as np\n",
    "\n",
    "def TRGfromDAT(dat_path, sampling_rate=20000, CHANNEL_NUM = 61, CHANNEL_TRG = 0):\n",
    "    dat = np.fromfile(dat_path,dtype='h').reshape(-1,CHANNEL_NUM)\n",
    "    trg_column = dat[:,CHANNEL_TRG]\n",
    "    not_saturate = np.where(trg_column != trg_column.max())[0] #search unsaturated range \n",
    "    temp_trg = np.diff(not_saturate)\n",
    "    temp_trg_index = np.where(temp_trg != 1)[0] + 1 #search uncontinous index from unsaturated range \n",
    "    trg_index = np.r_[0,temp_trg_index]\n",
    "    trg_array = not_saturate[trg_index] / sampling_rate\n",
    "    return trg_array\n",
    "\n",
    "# raw dataから1ch分の情報を取得\n",
    "#import numpy as np\n",
    "\n",
    "def ReadRawFile(path, channel, CHANNEL_NUM = 61):\n",
    "    raw = np.fromfile(path,dtype='h').reshape(-1,CHANNEL_NUM)\n",
    "    # print('raw.shape', raw.shape)\n",
    "    return raw[:,CH_ARRAY==channel][:,0]\n",
    "\n",
    "\n",
    "def BandPassFilter(wave_raw, bottom=300, top=3000, sampling_rate=20000):\n",
    "    nyq = sampling_rate / 2\n",
    "    cutoff = np.array([bottom, top]) / nyq\n",
    "    numtaps = 255\n",
    "    bpf = firwin(numtaps, cutoff, pass_zero=False)\n",
    "    return lfilter(bpf, 1, wave_raw)[int((numtaps-1)/2):]\n",
    "\n",
    "def SpikeDetection(wave_filtered, sd_thr=4, order=15, spike=-1):\n",
    "    peaks = argrelmin(-1*spike*wave_filtered, order=order)[0]\n",
    "    #選別用の閾値の計算\n",
    "    ##median\n",
    "    median = np.median(wave_filtered)\n",
    "    threshold = median - sd_thr * (np.median(abs(wave_filtered - median)) / 0.6745)\n",
    "    # print('Threshold: ', threshold)\n",
    "    #スパイクの選別\n",
    "    spike_index = peaks[wave_filtered[peaks] < threshold]\n",
    "    # print('Peak number: ', spike_index.size)\n",
    "    return spike_index\n",
    "\n",
    "\n",
    "def MakeWaveShape(temp_wave_array):\n",
    "    return np.arange(temp_wave_array[0], temp_wave_array[1])\n",
    "\n",
    "def GetWaveShape(spike_index, wave_filtered, area_before_peak_ms=1, area_after_peak_ms=2, sampling_rate=20000, ms=1000):\n",
    "    area_before_peak_index = int(area_before_peak_ms * sampling_rate / ms)\n",
    "    area_after_peak_index = int(area_after_peak_ms * sampling_rate / ms)\n",
    "    temp_wave_array = np.c_[spike_index-area_before_peak_index, spike_index+area_after_peak_index]\n",
    "    wave_array = np.array(list(map(MakeWaveShape, temp_wave_array)))\n",
    "\n",
    "    unuse_peak_index_1 = np.where(wave_array[:,-1] > wave_filtered.size-40)[0]\n",
    "    unuse_peak_index_2 = np.where(wave_array[:,0] < 0)[0]\n",
    "    wave_array = np.delete(wave_array, np.r_[unuse_peak_index_1,unuse_peak_index_2], axis=0)\n",
    "    spike_index = np.delete(spike_index, np.r_[unuse_peak_index_1,unuse_peak_index_2], axis=0)\n",
    "\n",
    "    spike_shape = wave_filtered[wave_array]\n",
    "    # print('Wave_Shape.shape :', spike_shape.shape)\n",
    "    return spike_shape, spike_index\n",
    "\n",
    "def CutWaveShape(spike_shape, area=13):\n",
    "#     roi = np.arange(spike_shape.shape[1]/3 - area, spike_shape.shape[1]/3 + 2*area + 1).astype(np.int)\n",
    "    roi = np.arange(\n",
    "    spike_shape.shape[1] / 3 - area, \n",
    "    spike_shape.shape[1] / 3 + 2 * area + 1\n",
    ").astype(int)\n",
    "    return spike_shape[:,roi]\n",
    "\n",
    "def DimensionalityReductionWithDiff1(features, n_comp):\n",
    "    features_diff = np.diff(features, n=1)\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    X_pca = pca.fit_transform(features_diff)\n",
    "    return X_pca, pca.explained_variance_ratio_\n",
    "\n",
    "def DimensionalityReductionWithDiffs(features, n_comp):\n",
    "    features_diff = np.c_[np.diff(features, n=1), np.diff(features, n=2)]\n",
    "    pca = PCA(n_components=features.shape[1])\n",
    "    X_pca = pca.fit_transform(features_diff)\n",
    "    return X_pca[:,:int(n_comp)], pca.explained_variance_ratio_[:int(n_comp)]\n",
    "\n",
    "\n",
    "# def ClusteringWithHDBSCAN(spike_feature, clu_size=2500, min_sam=250,cor_num=4, lea_siz=100): \n",
    "#     try:\n",
    "#         clusters = HDBSCAN(min_cluster_size=clu_size, min_samples=min_sam, leaf_size=lea_siz,\n",
    "#                            cluster_selection_method='leaf',core_dist_n_jobs=cor_num).fit_predict(spike_feature)\n",
    "#         return clusters\n",
    "#     except ValueError:\n",
    "#         print('There was ValueError!! So now using eom!!!')\n",
    "#         hdbscan = HDBSCAN(min_cluster_size=10, min_samples=100, core_dist_n_jobs=cor_num,allow_single_cluster=True)\n",
    "#         hdbscan.fit(spike_feature)\n",
    "#         clusters = hdbscan.labels_\n",
    "#         clusters[hdbscan.probabilities_ < 0.3] = -1\n",
    "#         return clusters\n",
    "#     except:\n",
    "#         print('Any Error Were Occured!!!')\n",
    "#         return 0\n",
    "\n",
    "def ClusteringWithHDBSCAN(spike_feature, clu_size=2000, min_sam=250,cor_num=1, lea_siz=100): \n",
    "        try:\n",
    "            clusters = HDBSCAN(min_cluster_size=clu_size, min_samples=min_sam, leaf_size=lea_siz,\n",
    "                               cluster_selection_method='leaf',core_dist_n_jobs=cor_num).fit_predict(spike_feature)\n",
    "#             print(\"pjifea\")\n",
    "            if(np.unique(clusters).shape[0] == 1):\n",
    "                raise ValueError\n",
    "#             print(\"あ\")\n",
    "            return clusters\n",
    "        except ValueError:\n",
    "            try:\n",
    "                clusters = HDBSCAN(min_cluster_size=320, min_samples=10, leaf_size=lea_siz,\n",
    "                                        cluster_selection_method='leaf',core_dist_n_jobs=cor_num, allow_single_cluster=True).fit_predict(spike_feature)\n",
    "                if(np.unique(clusters).shape[0] == 1):\n",
    "                    raise ValueError\n",
    "#                 print(\"い\")\n",
    "                return clusters\n",
    "            except ValueError:\n",
    "                print('There was ValueError!! So now using eom!!!')\n",
    "                hdbscan = HDBSCAN(min_cluster_size=10, min_samples=100, core_dist_n_jobs=cor_num,allow_single_cluster=True)\n",
    "                hdbscan.fit(spike_feature)\n",
    "                clusters = hdbscan.labels_\n",
    "                clusters[hdbscan.probabilities_ < 0.3] = -1\n",
    "\n",
    "                return clusters\n",
    "            except:\n",
    "                print('Any Error Were Occured!!!')\n",
    "                return clusters\n",
    "\n",
    "        except:\n",
    "            print('Any Error Were Occured!!!')\n",
    "\n",
    "            return 0\n",
    "\n",
    "from scipy import signal\n",
    "import os\n",
    "\n",
    "\n",
    "def CalcACR(spike_time):\n",
    "    #変数の定義\n",
    "    window_auto = 1000\n",
    "    binWidth_auto = 1 #[ms]\n",
    "    #格納先の作成\n",
    "    bin_num = int(((window_auto * 2)/binWidth_auto) + 1)\n",
    "    hist_auto = np.zeros(bin_num)\n",
    "    isi_size = spike_time.size\n",
    "    ##ex) spike_time_all = [1,3,10,100,2100]\n",
    "    for mid_search in spike_time:\n",
    "        ##1週目\n",
    "        #mid_serch = 1\n",
    "        # left_end = 1 - 1000 = -999 (ms)\n",
    "        left_end = mid_search - window_auto\n",
    "        #right_end = 1 + 1000 = 1001 (ms)\n",
    "        right_end = mid_search + window_auto\n",
    "        # -999(ms) ~  1001(ms)の間に存在するスパイク発火のインデックスをindex_serchに格納\n",
    "        #該当するスパイク　 1, 3, 10, 100 　＊ spike_time_all内のスパイク発火時間\n",
    "        #index_search = 0,1,2,3\n",
    "        index_search = np.where((spike_time >= left_end)&(spike_time <= right_end))[0]\n",
    "        #spike_time_all[index_search] = 1,3,10\n",
    "        #temp_spike_time - 1 = 0, 2, 9\n",
    "        temp_spike_time = spike_time[index_search] - mid_search\n",
    "        #ヒストグラム書く\n",
    "        hist_auto += np.histogram(temp_spike_time, bins=bin_num, range=(-window_auto,window_auto))[0]\n",
    "\n",
    "    return hist_auto  #Norm_Hist_Auto\n",
    "##############################################\n",
    "\n",
    "def CalcPOW(acr,ex_file_path=''):\n",
    "    sampling_rate = 1000\n",
    "    freq, P = signal.periodogram(x=acr,fs=sampling_rate)\n",
    "    roi_x_pow = np.array([0,80])\n",
    "    \n",
    "    ##オシレーションインデックス算出\n",
    "    #Search_Area = np.array([5,15]) #5~10Hz\n",
    "    #PWR_ROI_Index = np.where((freq >= Search_Area[0]) & (PWR[:,0] < Search_Area[1]))[0]\n",
    "    #PWR_ROI = (np.max(PWR[PWR_ROI_Index,1]) - np.mean(PWR[PWR_ROI_Index,1]))/np.std(PWR[:,1])\n",
    "    #return np.max(PWR[PWR_ROI_Index,1]), PWR[np.argmax(PWR[PWR_ROI_Index,1])+PWR_ROI_Index[0],0], np.mean(PWR[:,1]), np.std(PWR[:,1]), PWR_ROI\n",
    "    \n",
    "    # figure_pow = plt.figure()\n",
    "    # pow_pointer = figure_pow.add_subplot(1,1,1)\n",
    "    # pow_pointer.plot(freq,P,color='black')\n",
    "    # pow_pointer.set_xlim(roi_x_pow)\n",
    "    # pow_pointer.set_xlabel('Frequency[Hz]')\n",
    "    # pow_pointer.set_ylabel('Power/frequency')\n",
    "    # # cell_id = os.path.split(ex_file_path)[1]\n",
    "    # pow_pointer.set_title(cell_id)\n",
    "    # #plt.ylim(0, 0.0007)\n",
    "\n",
    "    # plt.show()\n",
    "    # plt.clf()\n",
    "    # plt.close('all')\n",
    "    \n",
    "    return np.c_[freq,P]\n",
    "\n",
    "def JudgeAcr(xAxis, acr):\n",
    "    #クラスターが自己相関の基準を突破できるか判定\n",
    "    AllIndex = np.where((xAxis >= -200) & (xAxis <= 200))[0] #-200 ms ~ 200 msのインデックスを取得\n",
    "    SearchIndex =[998, 999, 1001, 1002] #-2,-1, 1,2 msのインデックスを取得\n",
    "\n",
    "    AllACR = acr[AllIndex]\n",
    "    SearchACR = acr[SearchIndex]\n",
    "\n",
    "    # 基準値算出\n",
    "    FireIndex = np.sum(SearchACR)/np.sum(AllACR)* 100 \n",
    "    return FireIndex\n",
    "\n",
    "def CalcCCR(spike_time1, spike_time2):\n",
    "    #変数の定義\n",
    "    #1000 ms\n",
    "    window_auto = 1000\n",
    "    #瓶サイズ\n",
    "    binWidth_auto = 1 #[ms]\n",
    "    #格納先の作成\n",
    "    #瓶の数 \n",
    "    #2000 / 1 + 1 = 2001\n",
    "    bin_num = int(((window_auto * 2)/binWidth_auto) + 1)\n",
    "    #2001個の0をhist_autoに格納\n",
    "    hist_auto = np.zeros(bin_num)\n",
    "    #全てのスパイク時間\n",
    "\n",
    "############################################################################################\n",
    "##変更点1： \n",
    "##①spike_time_all ⇨　cell1_spike_time_all，　\n",
    "##②さらにcell2_spike_time_all = isi[:,2]を追加　\n",
    "##\n",
    "############################################################################################\n",
    "    #スパイク発火の数\n",
    "    isi_size = spike_time1.size\n",
    "    ##ex) spike_time_all = [1,3,10,100,2100]\n",
    "    for mid_search in spike_time1:\n",
    "        ##1週目\n",
    "        #mid_serch = 1\n",
    "        # left_end = 1 - 1000 = -999 (ms)\n",
    "        left_end = mid_search - window_auto\n",
    "        #right_end = 1 + 1000 = 1001 (ms)\n",
    "        right_end = mid_search + window_auto\n",
    "        # -999(ms) ~  1001(ms)の間に存在するスパイク発火のインデックスをindex_serchに格納\n",
    "        #該当するスパイク　 1, 3, 10, 100 　＊ spike_time_all内のスパイク発火時間\n",
    "############################################################################################\n",
    "##変更点2：         i\n",
    "##①ndex_search = np.where((spike_time_all >= left_end)&(spike_time_all <= right_end))[0]\n",
    "##このspike_time_all ⇨　cell1_spike_time_all，に変更\n",
    "##② temp_spike_time = spike_time_all[index_search] - mid_search\n",
    "##このspike_time_all ⇨　cell1_spike_time_all，に変更\n",
    "############################################################################################\n",
    "        #index_search = 0,1,2,3\n",
    "        index_search = np.where((spike_time2 >= left_end)&(spike_time2 <= right_end))[0]\n",
    "        #spike_time_all[index_search] = 1,3,10\n",
    "        #temp_spike_time - 1 = 0, 2, 9\n",
    "        temp_spike_time = spike_time2[index_search] - mid_search\n",
    "        #ヒストグラム書く\n",
    "        hist_auto += np.histogram(temp_spike_time, bins=bin_num, range=(-window_auto,window_auto))[0]\n",
    "    return hist_auto #Norm_Hist_Auto\n",
    "##############################################\n",
    "\n",
    "def JudgeCcr(isi, PairList):\n",
    "    NewCluNosAfterAcr = isi[:, 3]\n",
    "    for pair in list(itertools.permutations((NewCluNosAfterAcr), 2)):\n",
    "        if pair in PairList:\n",
    "            continue\n",
    "        CluNo1, CluNo2 = pair\n",
    "        print(CluNo1, CluNo2)\n",
    "        TempClusters = isi[:, 3]\n",
    "        Tempindex1 = np.where(TempClusters == CluNo1)[0]\n",
    "        Tempindex2 = np.where(TempClusters == CluNo2)[0]\n",
    "\n",
    "        TempSpikeTime1 = isi[Tempindex1, 1]\n",
    "        TempSpikeTime2 = isi[Tempindex2, 1]\n",
    "\n",
    "        ccr = CalcCCR(TempSpikeTime1, TempSpikeTime2)\n",
    "\n",
    "        #オートコレロをもとにした基準値を算出\n",
    "        FireIndex = JudgeAcr(xAxis, ccr)\n",
    "        print(FireIndex)\n",
    "\n",
    "        if FireIndex < 1:\n",
    "            NewCluNo = np.max(CluNosAfterAcr) + 1\n",
    "            NewCluIndex = np.r_[Tempindex1, Tempindex2]\n",
    "            np.put(TempClusters, NewCluIndex, NewCluNo)\n",
    "            break\n",
    "    return isi, PairList\n",
    "\n",
    "def GetTemplates(waves):\n",
    "    template = np.mean(waves, axis=0)\n",
    "    template_sd = np.std(waves, axis=0)\n",
    "    return np.array([template, template_sd])\n",
    "\n",
    "def GetWaves(clu, result, wave_shape):\n",
    "    temp_index = np.where(result==clu)[0]\n",
    "    return wave_shape[temp_index,:]\n",
    "\n",
    "# クラスタリング結果を元に波形からテンプレートを作成\n",
    "def MakeTemplates(clu, result, wave_shape):\n",
    "    return GetTemplates(GetWaves(clu, result, wave_shape))\n",
    "\n",
    "def CheckTemplate(template, wave):\n",
    "    temp_late, temp_late_sd = template\n",
    "    temp_lower = wave > (temp_late-temp_late_sd)\n",
    "    temp_upper = wave < (temp_late+temp_late_sd)\n",
    "    temp_index = np.sum([temp_upper.T[11:14],  temp_lower.T[11:14]])\n",
    "    if(temp_index == 6):\n",
    "        #return np.array([np.sum([temp_lower, temp_upper]), temp_index]).astype(np.int)\n",
    "        return np.array([np.sum([temp_lower, temp_upper]), temp_index]).astype(int)\n",
    "\n",
    "    else:\n",
    "        #return np.array([0, temp_index]).astype(np.int)\n",
    "        return np.array([0, temp_index]).astype(int)\n",
    "\n",
    "\n",
    "# マージ結果を元にクラスタを更新\n",
    "def ChangeCluster(cluster, marges):\n",
    "    new_cluster = cluster.copy()\n",
    "    main_clus = marges[0]\n",
    "    cluss = np.unique(cluster)\n",
    "    for clus in np.unique(marges[1:]):\n",
    "        new_cluster[cluster==clus] =  main_clus\n",
    "    return new_cluster    \n",
    "\n",
    "def MargeCluster_TM(cluster, wave_shape, thr_marge=115):\n",
    "    #default value of thr_marge is set for waveform with 60 points.\n",
    "    new_cluster = cluster.copy()\n",
    "    clus_list = np.unique(cluster)[1:]\n",
    "    templates = np.array(list(map(functools.partial(MakeTemplates, result=cluster, wave_shape=wave_shape), clus_list)))\n",
    "    clus_score = np.zeros([clus_list.shape[0],clus_list.shape[0]])\n",
    "    for i in range(clus_list.shape[0]):\n",
    "        clus_score[:,i] = np.array(list(map(functools.partial(CheckTemplate, wave=templates[i,0]),templates)))[:,0]\n",
    "        clus_score[:i+1,i] = 0\n",
    "    marges = np.where(clus_score.flatten() >= thr_marge)[0]\n",
    "    marges = np.c_[marges%clus_list.shape[0], marges//clus_list.shape[0]]\n",
    "    if marges.shape[0] >= 1:\n",
    "        for marge in reversed(marges):\n",
    "            # print('marge clusters : ', marge)\n",
    "            new_cluster = ChangeCluster(new_cluster, marge)\n",
    "    # print(np.unique(new_cluster))\n",
    "    return new_cluster\n",
    "\n",
    "def ReclustNoise(noise_wave, templates, thr_socre=72):\n",
    "    clu = -1\n",
    "    clus_score = np.array(list(map(functools.partial(CheckTemplate, wave=noise_wave),templates)))\n",
    "    max_index = np.argmax(clus_score[:,0])\n",
    "    if((clus_score[max_index,0] > 72) & (clus_score[max_index,1] == 6)):\n",
    "        clu = max_index\n",
    "    return clu\n",
    "\n",
    "# noiseに分類されたspikeをtemplate-matchingによって救済\n",
    "def RescueNoise(cluster, wave_shape, thr_noise=72):\n",
    "    new_cluster = cluster.copy()\n",
    "    ori_clus = np.roll(np.unique(new_cluster), -1)\n",
    "    noise_index = np.where(cluster==-1)[0]\n",
    "    templates = np.array(list(map(functools.partial(MakeTemplates, result=cluster, wave_shape=wave_shape), np.unique(cluster)[1:])))\n",
    "    noise_waves = GetWaves(-1, cluster, wave_shape)\n",
    "    noise_reclust = np.array(list(map(functools.partial(ReclustNoise, templates = templates), noise_waves)))\n",
    "    new_cluster[noise_index] = ori_clus[noise_reclust]\n",
    "    return new_cluster\n",
    "\n",
    "# raw dataに格納されている情報の列方向の順番\n",
    "# CH_ARRAY = np.array(['trigger', 'ch47', 'ch48', 'ch46', 'ch45', 'ch38', 'ch37', 'ch28',\n",
    "# 'ch36', 'ch27', 'ch17', 'ch26', 'ch16', 'ch35', 'ch25', 'ch15', 'ch14', 'ch24', 'ch34',\n",
    "# 'ch13', 'ch23', 'ch12', 'ch22', 'ch33', 'ch21', 'ch32', 'ch31', 'ch44', 'ch43', 'ch41',\n",
    "# 'ch42', 'ch52', 'ch51', 'ch53', 'ch54', 'ch61', 'ch62', 'ch71', 'ch63', 'ch72', 'ch82',\n",
    "# 'ch73', 'ch83', 'ch64', 'ch74', 'ch84', 'ch85', 'ch75', 'ch65', 'ch86', 'ch76', 'ch87',\n",
    "# 'ch77', 'ch66', 'ch78', 'ch67', 'ch68', 'ch55', 'ch56', 'ch58', 'ch57'])\n",
    "\n",
    "CH_ARRAY = np.array(['trigger', 'ch21', 'ch31', 'ch41', 'ch51', 'ch61', 'ch71', 'ch12',\n",
    "'ch22', 'ch32', 'ch42', 'ch52', 'ch62', 'ch72', 'ch82', 'ch13', 'ch23', 'ch33', 'ch43',\n",
    "'ch53', 'ch63', 'ch73', 'ch83', 'ch14', 'ch24', 'ch34', 'ch44', 'ch54', 'ch64', 'ch74',\n",
    "'ch84', 'ch15', 'ch25', 'ch35', 'ch45', 'ch55', 'ch65', 'ch75', 'ch85', 'ch16', 'ch26',\n",
    "'ch36', 'ch46', 'ch56', 'ch66', 'ch76', 'ch86', 'ch17', 'ch27', 'ch37', 'ch47', 'ch57',\n",
    "'ch67', 'ch77', 'ch87', 'ch28', 'ch38', 'ch48', 'ch58', 'ch68', 'ch78'])\n",
    "\n",
    "## 可視化する時のファイル形式\n",
    "PIC_EXT = '.png'\n",
    "\n",
    "## 可視化する際のグラフの線の色\n",
    "COLOR = ['b', 'chartreuse', 'r', 'c', 'm', 'y', 'k', 'Brown', 'ForestGreen', 'darkcyan', 'maroon', 'orange', 'green', 'steelblue', 'purple', 'gold', 'navy', 'gray', 'indigo', 'black', 'darkgoldenrod']\n",
    "\n",
    "## secondからmili secondへの変換\n",
    "MS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa48503d",
   "metadata": {},
   "source": [
    "# 関数化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5953cfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_path C:/Users/Imaris/Desktop/watanabe/250801/リポジトリ/test2ch.raw\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "raw_path = \"C:/Users/Imaris/Desktop/watanabe/250801/リポジトリ/test2ch.raw\"\n",
    "print(\"raw_path\", raw_path)\n",
    "\n",
    "# 変数\n",
    "data_name = os.path.basename(raw_path).split(\".\")[0]\n",
    "channels = [\"ch21\", \"ch55\"]\n",
    "channel_num = len(channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c5c575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "def load_raw_matrix(path: \"path\", n_channels: int, dtype=\"h\", scale=10.0) -> np.ndarray:\n",
    "    \"\"\"RAW -> (n_samples, n_channels) の 2 次元配列に変換\"\"\"\n",
    "    data = np.fromfile(path, dtype=dtype)\n",
    "    if data.size % n_channels:\n",
    "        raise ValueError(f\"RAW size {data.size} not divisible by n_channels={n_channels}\")\n",
    "    return data.reshape(-1, n_channels).astype(np.float32) / scale\n",
    "\n",
    "\n",
    "def extract_channel(\n",
    "    raw_matrix: np.ndarray,\n",
    "    ch: int,\n",
    "    ch_array:  Optional[np.ndarray] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    raw_matrix: load_raw_matrix で得た 2D 配列\n",
    "    ch        : 取り出したいチャンネル番号 (0-based)\n",
    "    ch_array  : 旧コードと同じ 'ch47', 'ch48'... などの順序を保持している配列（任意）\n",
    "    \"\"\"\n",
    "    if raw_matrix.ndim != 2:\n",
    "        raise ValueError(\"raw_matrix must be 2D\")\n",
    "\n",
    "    if raw_matrix.shape[1] == 1:\n",
    "        # 1 ch RAW の場合はそのまま返す\n",
    "        return raw_matrix[:, 0]\n",
    "\n",
    "    if ch_array is None:\n",
    "        # RAW が単純な 0..N-1 の並びなら ch を直接使う\n",
    "        if ch < 0 or ch >= raw_matrix.shape[1]:\n",
    "            raise IndexError(f\"channel {ch} out of range (0..{raw_matrix.shape[1]-1})\")\n",
    "        return raw_matrix[:, ch]\n",
    "\n",
    "    # 旧ノートのように CH_ARRAY で並びを決めている場合\n",
    "    mask = ch_array == ch\n",
    "    if not np.any(mask):\n",
    "        raise ValueError(f\"{ch} は CH_ARRAY に存在しません\")\n",
    "    return raw_matrix[:, mask][:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02f91a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #入力データの準備\n",
    "# raw_matrix = load_raw_matrix(raw_path, channel_num)\n",
    "# wave = extract_channel(raw_matrix, ch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5ae172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ChannelMeta:\n",
    "    data_name: str\n",
    "    channel_label: str  # e.g. 'ch000'\n",
    "    figure_dirs: dict[str, Path]  # 'spike_detect', 'pca', 'sorting_cluster', 'auto_correlo'\n",
    "    h5_path: Path\n",
    "    log_path: Path\n",
    "    sampling_rate: int\n",
    "\n",
    "\n",
    "def process_channel(raw_wave: np.ndarray, cfg) -> dict:\n",
    "    \"\"\"\n",
    "    1 チャネル分の波形を受け取り、フィルタ〜クラスタリングまで処理する。\n",
    "    戻り値は後段の可視化/保存で使う dict。\n",
    "    \"\"\"\n",
    "    filtered = BandPassFilter(\n",
    "        raw_wave,\n",
    "        bottom=cfg.band_bottom,\n",
    "        top=cfg.band_top,\n",
    "        sampling_rate=cfg.fs,\n",
    "    )\n",
    "    spike_idx = SpikeDetection(\n",
    "        filtered,\n",
    "        sd_thr=cfg.spike_threshold_sd,\n",
    "        order=cfg.spike_order,\n",
    "        spike=cfg.spike_polarity,\n",
    "    )\n",
    "\n",
    "    waveforms, spike_idx = GetWaveShape(\n",
    "        spike_idx,\n",
    "        filtered,\n",
    "        area_before_peak_ms=cfg.window_before_ms,\n",
    "        area_after_peak_ms=cfg.window_after_ms,\n",
    "        sampling_rate=cfg.fs,\n",
    "        ms=MS,\n",
    "    )\n",
    "    if spike_idx.size == 0:\n",
    "        return {\n",
    "            \"filtered\": filtered,\n",
    "            \"spike_idx\": spike_idx,\n",
    "            \"waveforms\": np.empty((0, 0)),\n",
    "            \"waveforms_roi\": np.empty((0, 0)),\n",
    "            \"features\": np.empty((0, cfg.pca_components)),\n",
    "            \"variance\": np.zeros(cfg.pca_components),\n",
    "            \"labels\": np.array([], dtype=int),\n",
    "            \"spike_times_ms\": np.array([]),\n",
    "        }\n",
    "\n",
    "    waveforms_roi = CutWaveShape(waveforms, area=cfg.cut_area)\n",
    "    x_pca, variance = DimensionalityReductionWithDiffs(waveforms_roi, cfg.pca_components)\n",
    "    features = StandardScaler().fit_transform(x_pca)\n",
    "\n",
    "    clusters = ClusteringWithHDBSCAN(features, clu_size=cfg.cluster_min_size, min_sam=cfg.cluster_min_samples)\n",
    "    merged = MargeCluster_TM(cluster=clusters, wave_shape=waveforms, thr_marge=cfg.template_merge_score)\n",
    "    refined = RescueNoise(\n",
    "        cluster=merged,\n",
    "        wave_shape=CutWaveShape(waveforms, area=cfg.cut_area),\n",
    "        thr_noise=cfg.noise_reassign_score,\n",
    "    )\n",
    "\n",
    "    spike_times_ms = spike_idx / (cfg.fs / MS)\n",
    "    isi = np.c_[np.arange(1, spike_times_ms.size + 1), spike_times_ms, np.diff(np.r_[0, spike_times_ms]), refined]\n",
    "\n",
    "    return {\n",
    "        \"filtered\": filtered,\n",
    "        \"spike_idx\": spike_idx,\n",
    "        \"waveforms\": waveforms,\n",
    "        \"waveforms_roi\": waveforms_roi,\n",
    "        \"features\": features,\n",
    "        \"variance\": variance,\n",
    "        \"labels\": refined,\n",
    "        \"spike_times_ms\": spike_times_ms,\n",
    "        \"isi\": isi,\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_and_save(meta: ChannelMeta, result: dict) -> None:\n",
    "    \"\"\"\n",
    "    process_channel の戻り値を受け取り、図を保存しつつ HDF5 とログに結果をまとめる。\n",
    "    \"\"\"\n",
    "    filtered = result[\"filtered\"]\n",
    "    spike_idx = result[\"spike_idx\"]\n",
    "    waveforms = result[\"waveforms\"]\n",
    "    waveforms_roi = result[\"waveforms_roi\"]\n",
    "    features = result[\"features\"]\n",
    "    labels = result[\"labels\"]\n",
    "    spike_times_ms = result[\"spike_times_ms\"]\n",
    "    variance = result[\"variance\"]\n",
    "\n",
    "    total_spikes = int(spike_idx.size)\n",
    "    if labels.size:\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    else:\n",
    "        unique_labels = np.array([], dtype=int)\n",
    "        counts = np.array([], dtype=int)\n",
    "    positive_labels = unique_labels[unique_labels >= 0]\n",
    "\n",
    "    log_lines = [\n",
    "        f\"[{meta.channel_label}]\",\n",
    "        f\"total_spikes: {total_spikes}\",\n",
    "        f\"detected_clusters: {int(positive_labels.size)}\",\n",
    "    ]\n",
    "    if variance.size and total_spikes:\n",
    "        log_lines.append(\"pca_variance: \" + \", \".join(f\"{v:.4f}\" for v in np.atleast_1d(variance)))\n",
    "    if unique_labels.size:\n",
    "        log_lines.append(\"cluster_counts:\")\n",
    "        for clu, count in zip(unique_labels, counts):\n",
    "            label_name = f\"cluster {int(clu)}\" if clu >= 0 else \"noise\"\n",
    "            log_lines.append(f\"  {label_name}: {int(count)}\")\n",
    "    else:\n",
    "        log_lines.append(\"cluster_counts: none\")\n",
    "    acr_logs = []\n",
    "\n",
    "    # 生波形の可視化\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    t = np.arange(filtered.size) / meta.sampling_rate\n",
    "    plt.plot(t, filtered, lw=0.5, color=\"steelblue\")\n",
    "    if spike_idx.size:\n",
    "        plt.plot(spike_idx / meta.sampling_rate, filtered[spike_idx], \"r.\", ms=3)\n",
    "    plt.xlabel(\"Time [s]\")\n",
    "    plt.ylabel(\"Filtered (uV)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(meta.figure_dirs[\"spike_detect\"] / f\"{meta.data_name}_{meta.channel_label}_spike_detect.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # PCA 図\n",
    "    if features.size:\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.scatter(features[:, 0], features[:, 1], s=5, c=\"gray\", alpha=0.5)\n",
    "        plt.xlabel(\"PC1\")\n",
    "        plt.ylabel(\"PC2\")\n",
    "        plt.title(f\"PCA scatter {meta.data_name}_{meta.channel_label}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"pca\"] / f\"{meta.data_name}_{meta.channel_label}_pca_raw.png\")\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        for clu, color in zip(unique_labels, itertools.cycle(COLOR)):\n",
    "            mask = labels == clu\n",
    "            if not mask.any():\n",
    "                continue\n",
    "            plt.scatter(features[mask, 0], features[mask, 1], s=8, alpha=0.7, label=f\"clu {clu}\", c=color)\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.xlabel(\"PC1\")\n",
    "        plt.ylabel(\"PC2\")\n",
    "        plt.title(f\"PCA clustered {meta.data_name}_{meta.channel_label}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"pca\"] / f\"{meta.data_name}_{meta.channel_label}_pca_cluster.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # クラスタ波形\n",
    "    for clu, color in zip(unique_labels, itertools.cycle(COLOR)):\n",
    "        mask = labels == clu\n",
    "        if clu < 0 or not mask.any():\n",
    "            continue\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.plot(waveforms[mask].T, color=color, alpha=0.2, lw=0.5)\n",
    "        plt.plot(np.median(waveforms[mask], axis=0), color=color, lw=2)\n",
    "        plt.title(f\"{meta.channel_label} cluster {clu} (n={mask.sum()})\")\n",
    "        plt.xlabel(\"Samples\")\n",
    "        plt.ylabel(\"uV\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"sorting_cluster\"] / f\"{meta.data_name}_{meta.channel_label}_cluster{clu}_waveforms.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # ACR とパワースペクトル\n",
    "    x_axis = np.arange(-1000, 1001)\n",
    "    for clu in unique_labels:\n",
    "        mask = labels == clu\n",
    "        if clu < 0 or not mask.any():\n",
    "            continue\n",
    "        acr = CalcACR(spike_times_ms[mask])\n",
    "        fire_index = JudgeAcr(x_axis, acr)\n",
    "        acr_logs.append(\n",
    "            f\"  cluster {int(clu)}: n={int(mask.sum())}, fire_index={fire_index:.3f}% ({'PASS' if fire_index <= 1.0 else 'FAIL'})\"\n",
    "        )\n",
    "\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.plot(x_axis, acr, color=\"black\", lw=1)\n",
    "        plt.xlim(-200, 200)\n",
    "        plt.xlabel(\"Time lag [ms]\")\n",
    "        plt.ylabel(\"Autocorrelation\")\n",
    "        plt.title(f\"{meta.channel_label} cluster {clu} autocorrelogram\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"auto_correlo\"] / f\"{meta.data_name}_{meta.channel_label}_cluster{clu}_acr.png\")\n",
    "        plt.close()\n",
    "\n",
    "        freqP = CalcPOW(acr, ex_file_path=\"\")\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.plot(freqP[:, 0], freqP[:, 1], color=\"black\", lw=1)\n",
    "        plt.xlim(0, 80)\n",
    "        plt.xlabel(\"Frequency [Hz]\")\n",
    "        plt.ylabel(\"Power/frequency\")\n",
    "        plt.title(f\"{meta.channel_label} cluster {clu} power spectrum\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"auto_correlo\"] / f\"{meta.data_name}_{meta.channel_label}_cluster{clu}_power.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # HDF5 保存\n",
    "    datasets = {\n",
    "        \"labels\": labels,\n",
    "        \"spike_times_ms\": spike_times_ms,\n",
    "        \"waveforms\": waveforms,\n",
    "        \"waveforms_roi\": waveforms_roi,\n",
    "        \"variance\": variance,\n",
    "    }\n",
    "    with h5py.File(meta.h5_path, \"a\") as h5:\n",
    "        group = h5.require_group(meta.channel_label)\n",
    "        for name, data in datasets.items():\n",
    "            if name in group:\n",
    "                del group[name]\n",
    "            if data.size:\n",
    "                group.create_dataset(name, data=data, compression=\"gzip\", compression_opts=4)\n",
    "            else:\n",
    "                group.create_dataset(name, shape=data.shape, dtype=data.dtype)\n",
    "\n",
    "    if acr_logs:\n",
    "        log_lines.append(\"acr_tests:\")\n",
    "        log_lines.extend(acr_logs)\n",
    "    else:\n",
    "        log_lines.append(\"acr_tests: n/a\")\n",
    "\n",
    "    with meta.log_path.open(\"a\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(\"\\n\".join(log_lines) + \"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2df7b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ChannelMeta:\n",
    "    data_name: str\n",
    "    channel_label: str  # e.g. 'ch000'\n",
    "    figure_dirs: dict[str, Path]  # 'spike_detect', 'pca', 'sorting_cluster', 'auto_correlo'\n",
    "    h5_path: Path\n",
    "    log_path: Path\n",
    "    sampling_rate: int\n",
    "\n",
    "\n",
    "def process_channel(raw_wave: np.ndarray, cfg) -> dict:\n",
    "    \"\"\"\n",
    "    1 チャンネル分の波形からクラスタリング結果までをまとめて計算。\n",
    "    戻り値は可視化・保存で使う情報を dict で返す。\n",
    "    \"\"\"\n",
    "    filtered = BandPassFilter(\n",
    "        raw_wave,\n",
    "        bottom=cfg.band_bottom,\n",
    "        top=cfg.band_top,\n",
    "        sampling_rate=cfg.fs,\n",
    "    )\n",
    "    spike_idx = SpikeDetection(\n",
    "        filtered,\n",
    "        sd_thr=cfg.spike_threshold_sd,\n",
    "        order=cfg.spike_order,\n",
    "        spike=cfg.spike_polarity,\n",
    "    )\n",
    "\n",
    "    waveforms, spike_idx = GetWaveShape(\n",
    "        spike_idx,\n",
    "        filtered,\n",
    "        area_before_peak_ms=cfg.window_before_ms,\n",
    "        area_after_peak_ms=cfg.window_after_ms,\n",
    "        sampling_rate=cfg.fs,\n",
    "        ms=MS,\n",
    "    )\n",
    "    if spike_idx.size == 0:\n",
    "        return {\n",
    "            \"filtered\": filtered,\n",
    "            \"spike_idx\": spike_idx,\n",
    "            \"waveforms\": np.empty((0, 0)),\n",
    "            \"waveforms_roi\": np.empty((0, 0)),\n",
    "            \"features\": np.empty((0, cfg.pca_components)),\n",
    "            \"variance\": np.zeros(cfg.pca_components),\n",
    "            \"labels\": np.array([], dtype=int),\n",
    "            \"spike_times_ms\": np.array([]),\n",
    "        }\n",
    "\n",
    "    waveforms_roi = CutWaveShape(waveforms, area=cfg.cut_area)\n",
    "    x_pca, variance = DimensionalityReductionWithDiffs(waveforms_roi, cfg.pca_components)\n",
    "    features = StandardScaler().fit_transform(x_pca)\n",
    "\n",
    "    clusters = ClusteringWithHDBSCAN(features, clu_size=cfg.cluster_min_size, min_sam=cfg.cluster_min_samples)\n",
    "    merged = MargeCluster_TM(cluster=clusters, wave_shape=waveforms, thr_marge=cfg.template_merge_score)\n",
    "    refined = RescueNoise(\n",
    "        cluster=merged,\n",
    "        wave_shape=CutWaveShape(waveforms, area=cfg.cut_area),\n",
    "        thr_noise=cfg.noise_reassign_score,\n",
    "    )\n",
    "\n",
    "    spike_times_ms = spike_idx / (cfg.fs / MS)\n",
    "    isi = np.c_[np.arange(1, spike_times_ms.size + 1), spike_times_ms, np.diff(np.r_[0, spike_times_ms]), refined]\n",
    "\n",
    "    return {\n",
    "        \"filtered\": filtered,\n",
    "        \"spike_idx\": spike_idx,\n",
    "        \"waveforms\": waveforms,\n",
    "        \"waveforms_roi\": waveforms_roi,\n",
    "        \"features\": features,\n",
    "        \"variance\": variance,\n",
    "        \"labels\": refined,\n",
    "        \"spike_times_ms\": spike_times_ms,\n",
    "        \"isi\": isi,\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_and_save(meta: ChannelMeta, result: dict) -> None:\n",
    "    \"\"\"\n",
    "    Receive process_channel output, produce plots, and persist results in HDF5.\n",
    "    \"\"\"\n",
    "    filtered = result[\"filtered\"]\n",
    "    spike_idx = result[\"spike_idx\"]\n",
    "    waveforms = result[\"waveforms\"]\n",
    "    waveforms_roi = result[\"waveforms_roi\"]\n",
    "    features = result[\"features\"]\n",
    "    labels = result[\"labels\"]\n",
    "    spike_times_ms = result[\"spike_times_ms\"]\n",
    "    variance = result[\"variance\"]\n",
    "\n",
    "    # 検出波形\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    t = np.arange(filtered.size) / meta.sampling_rate\n",
    "    plt.plot(t, filtered, lw=0.5, color=\"steelblue\")\n",
    "    if spike_idx.size:\n",
    "        plt.plot(spike_idx / meta.sampling_rate, filtered[spike_idx], \"r.\", ms=3)\n",
    "    plt.xlabel(\"Time [s]\")\n",
    "    plt.ylabel(\"Filtered (uV)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(meta.figure_dirs[\"spike_detect\"] / f\"{meta.data_name}_{meta.channel_label}_spike_detect.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # PCA 図\n",
    "    if features.size:\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.scatter(features[:, 0], features[:, 1], s=5, c=\"gray\", alpha=0.5)\n",
    "        plt.xlabel(\"PC1\")\n",
    "        plt.ylabel(\"PC2\")\n",
    "        plt.title(f\"PCA scatter {meta.data_name}_{meta.channel_label}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"pca\"] / f\"{meta.data_name}_{meta.channel_label}_pca_raw.png\")\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        for clu, color in zip(np.unique(labels), itertools.cycle(COLOR)):\n",
    "            mask = labels == clu\n",
    "            plt.scatter(features[mask, 0], features[mask, 1], s=8, alpha=0.7, label=f\"clu {clu}\", c=color)\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.xlabel(\"PC1\")\n",
    "        plt.ylabel(\"PC2\")\n",
    "        plt.title(f\"PCA clustered {meta.data_name}_{meta.channel_label}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"pca\"] / f\"{meta.data_name}_{meta.channel_label}_pca_cluster.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # クラスタ波形\n",
    "    for clu, color in zip(np.unique(labels), itertools.cycle(COLOR)):\n",
    "        mask = labels == clu\n",
    "        if clu < 0 or not mask.any():\n",
    "            continue\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.plot(waveforms[mask].T, color=color, alpha=0.2, lw=0.5)\n",
    "        plt.plot(np.median(waveforms[mask], axis=0), color=color, lw=2)\n",
    "        plt.title(f\"{meta.channel_label} cluster {clu} (n={mask.sum()})\")\n",
    "        plt.xlabel(\"Samples\")\n",
    "        plt.ylabel(\"uV\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"sorting_cluster\"] / f\"{meta.data_name}_{meta.channel_label}_cluster{clu}_waveforms.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # ACR とパワースペクトラム\n",
    "    x_axis = np.arange(-1000, 1001)\n",
    "    for clu in np.unique(labels):\n",
    "        mask = labels == clu\n",
    "        if clu < 0 or not mask.any():\n",
    "            continue\n",
    "        acr = CalcACR(spike_times_ms[mask])\n",
    "\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.plot(x_axis, acr, color=\"black\", lw=1)\n",
    "        plt.xlim(-200, 200)\n",
    "        plt.xlabel(\"Time lag [ms]\")\n",
    "        plt.ylabel(\"Autocorrelation\")\n",
    "        plt.title(f\"{meta.channel_label} cluster {clu} autocorrelogram\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"auto_correlo\"] / f\"{meta.data_name}_{meta.channel_label}_cluster{clu}_acr.png\")\n",
    "        plt.close()\n",
    "\n",
    "        freqP = CalcPOW(acr, ex_file_path=\"\")\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.plot(freqP[:, 0], freqP[:, 1], color=\"black\", lw=1)\n",
    "        plt.xlim(0, 80)\n",
    "        plt.xlabel(\"Frequency [Hz]\")\n",
    "        plt.ylabel(\"Power/frequency\")\n",
    "        plt.title(f\"{meta.channel_label} cluster {clu} power spectrum\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"auto_correlo\"] / f\"{meta.data_name}_{meta.channel_label}_cluster{clu}_power.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # HDF5 保存\n",
    "    datasets = {\n",
    "        \"labels\": labels,\n",
    "        \"spike_times_ms\": spike_times_ms,\n",
    "        \"waveforms\": waveforms,\n",
    "        \"waveforms_roi\": waveforms_roi,\n",
    "        \"variance\": variance,\n",
    "    }\n",
    "    with h5py.File(meta.h5_path, \"a\") as h5:\n",
    "        group = h5.require_group(meta.channel_label)\n",
    "        for name, data in datasets.items():\n",
    "            if name in group:\n",
    "                del group[name]\n",
    "            if data.size:\n",
    "                group.create_dataset(name, data=data, compression=\"gzip\", compression_opts=4)\n",
    "            else:\n",
    "                group.create_dataset(name, shape=data.shape, dtype=data.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93477678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ChannelMeta:\n",
    "    data_name: str\n",
    "    channel_label: str  # e.g. 'ch000'\n",
    "    figure_dirs: dict[str, Path]  # 'spike_detect', 'pca', 'sorting_cluster', 'auto_correlo'\n",
    "    h5_path: Path\n",
    "    log_path: Path\n",
    "    sampling_rate: int\n",
    "\n",
    "\n",
    "def process_channel(raw_wave: np.ndarray, cfg) -> dict:\n",
    "    \"\"\"\n",
    "    1 チャンネル分の波形からクラスタリング結果までをまとめて計算。\n",
    "    戻り値は可視化・保存で使う情報を dict で返す。\n",
    "    \"\"\"\n",
    "    filtered = BandPassFilter(\n",
    "        raw_wave,\n",
    "        bottom=cfg.band_bottom,\n",
    "        top=cfg.band_top,\n",
    "        sampling_rate=cfg.fs,\n",
    "    )\n",
    "    spike_idx = SpikeDetection(\n",
    "        filtered,\n",
    "        sd_thr=cfg.spike_threshold_sd,\n",
    "        order=cfg.spike_order,\n",
    "        spike=cfg.spike_polarity,\n",
    "    )\n",
    "\n",
    "    waveforms, spike_idx = GetWaveShape(\n",
    "        spike_idx,\n",
    "        filtered,\n",
    "        area_before_peak_ms=cfg.window_before_ms,\n",
    "        area_after_peak_ms=cfg.window_after_ms,\n",
    "        sampling_rate=cfg.fs,\n",
    "        ms=MS,\n",
    "    )\n",
    "    if spike_idx.size == 0:\n",
    "        return {\n",
    "            \"filtered\": filtered,\n",
    "            \"spike_idx\": spike_idx,\n",
    "            \"waveforms\": np.empty((0, 0)),\n",
    "            \"waveforms_roi\": np.empty((0, 0)),\n",
    "            \"features\": np.empty((0, cfg.pca_components)),\n",
    "            \"variance\": np.zeros(cfg.pca_components),\n",
    "            \"labels\": np.array([], dtype=int),\n",
    "            \"spike_times_ms\": np.array([]),\n",
    "        }\n",
    "\n",
    "    waveforms_roi = CutWaveShape(waveforms, area=cfg.cut_area)\n",
    "    x_pca, variance = DimensionalityReductionWithDiffs(waveforms_roi, cfg.pca_components)\n",
    "    features = StandardScaler().fit_transform(x_pca)\n",
    "\n",
    "    clusters = ClusteringWithHDBSCAN(features, clu_size=cfg.cluster_min_size, min_sam=cfg.cluster_min_samples)\n",
    "    merged = MargeCluster_TM(cluster=clusters, wave_shape=waveforms, thr_marge=cfg.template_merge_score)\n",
    "    refined = RescueNoise(\n",
    "        cluster=merged,\n",
    "        wave_shape=CutWaveShape(waveforms, area=cfg.cut_area),\n",
    "        thr_noise=cfg.noise_reassign_score,\n",
    "    )\n",
    "\n",
    "    spike_times_ms = spike_idx / (cfg.fs / MS)\n",
    "    isi = np.c_[np.arange(1, spike_times_ms.size + 1), spike_times_ms, np.diff(np.r_[0, spike_times_ms]), refined]\n",
    "\n",
    "    return {\n",
    "        \"filtered\": filtered,\n",
    "        \"spike_idx\": spike_idx,\n",
    "        \"waveforms\": waveforms,\n",
    "        \"waveforms_roi\": waveforms_roi,\n",
    "        \"features\": features,\n",
    "        \"variance\": variance,\n",
    "        \"labels\": refined,\n",
    "        \"spike_times_ms\": spike_times_ms,\n",
    "        \"isi\": isi,\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_and_save(meta: ChannelMeta, result: dict) -> None:\n",
    "    \"\"\"\n",
    "    Receive process_channel output, produce plots, and persist results in HDF5.\n",
    "    \"\"\"\n",
    "    filtered = result[\"filtered\"]\n",
    "    spike_idx = result[\"spike_idx\"]\n",
    "    waveforms = result[\"waveforms\"]\n",
    "    waveforms_roi = result[\"waveforms_roi\"]\n",
    "    features = result[\"features\"]\n",
    "    labels = result[\"labels\"]\n",
    "    spike_times_ms = result[\"spike_times_ms\"]\n",
    "    variance = result[\"variance\"]\n",
    "\n",
    "    # 検出波形\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    t = np.arange(filtered.size) / meta.sampling_rate\n",
    "    plt.plot(t, filtered, lw=0.5, color=\"steelblue\")\n",
    "    if spike_idx.size:\n",
    "        plt.plot(spike_idx / meta.sampling_rate, filtered[spike_idx], \"r.\", ms=3)\n",
    "    plt.xlabel(\"Time [s]\")\n",
    "    plt.ylabel(\"Filtered (uV)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(meta.figure_dirs[\"spike_detect\"] / f\"{meta.data_name}_{meta.channel_label}_spike_detect.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # PCA 図\n",
    "    if features.size:\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.scatter(features[:, 0], features[:, 1], s=5, c=\"gray\", alpha=0.5)\n",
    "        plt.xlabel(\"PC1\")\n",
    "        plt.ylabel(\"PC2\")\n",
    "        plt.title(f\"PCA scatter {meta.data_name}_{meta.channel_label}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"pca\"] / f\"{meta.data_name}_{meta.channel_label}_pca_raw.png\")\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        for clu, color in zip(np.unique(labels), itertools.cycle(COLOR)):\n",
    "            mask = labels == clu\n",
    "            plt.scatter(features[mask, 0], features[mask, 1], s=8, alpha=0.7, label=f\"clu {clu}\", c=color)\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.xlabel(\"PC1\")\n",
    "        plt.ylabel(\"PC2\")\n",
    "        plt.title(f\"PCA clustered {meta.data_name}_{meta.channel_label}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"pca\"] / f\"{meta.data_name}_{meta.channel_label}_pca_cluster.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # クラスタ波形\n",
    "    for clu, color in zip(np.unique(labels), itertools.cycle(COLOR)):\n",
    "        mask = labels == clu\n",
    "        if clu < 0 or not mask.any():\n",
    "            continue\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.plot(waveforms[mask].T, color=color, alpha=0.2, lw=0.5)\n",
    "        plt.plot(np.median(waveforms[mask], axis=0), color=color, lw=2)\n",
    "        plt.title(f\"{meta.channel_label} cluster {clu} (n={mask.sum()})\")\n",
    "        plt.xlabel(\"Samples\")\n",
    "        plt.ylabel(\"uV\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"sorting_cluster\"] / f\"{meta.data_name}_{meta.channel_label}_cluster{clu}_waveforms.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # ACR とパワースペクトラム\n",
    "    x_axis = np.arange(-1000, 1001)\n",
    "    for clu in np.unique(labels):\n",
    "        mask = labels == clu\n",
    "        if clu < 0 or not mask.any():\n",
    "            continue\n",
    "        acr = CalcACR(spike_times_ms[mask])\n",
    "\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.plot(x_axis, acr, color=\"black\", lw=1)\n",
    "        plt.xlim(-200, 200)\n",
    "        plt.xlabel(\"Time lag [ms]\")\n",
    "        plt.ylabel(\"Autocorrelation\")\n",
    "        plt.title(f\"{meta.channel_label} cluster {clu} autocorrelogram\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"auto_correlo\"] / f\"{meta.data_name}_{meta.channel_label}_cluster{clu}_acr.png\")\n",
    "        plt.close()\n",
    "\n",
    "        freqP = CalcPOW(acr, ex_file_path=\"\")\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.plot(freqP[:, 0], freqP[:, 1], color=\"black\", lw=1)\n",
    "        plt.xlim(0, 80)\n",
    "        plt.xlabel(\"Frequency [Hz]\")\n",
    "        plt.ylabel(\"Power/frequency\")\n",
    "        plt.title(f\"{meta.channel_label} cluster {clu} power spectrum\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"auto_correlo\"] / f\"{meta.data_name}_{meta.channel_label}_cluster{clu}_power.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # HDF5 保存\n",
    "    datasets = {\n",
    "        \"labels\": labels,\n",
    "        \"spike_times_ms\": spike_times_ms,\n",
    "        \"waveforms\": waveforms,\n",
    "        \"waveforms_roi\": waveforms_roi,\n",
    "        \"variance\": variance,\n",
    "    }\n",
    "    with h5py.File(meta.h5_path, \"a\") as h5:\n",
    "        group = h5.require_group(meta.channel_label)\n",
    "        for name, data in datasets.items():\n",
    "            if name in group:\n",
    "                del group[name]\n",
    "            if data.size:\n",
    "                group.create_dataset(name, data=data, compression=\"gzip\", compression_opts=4)\n",
    "            else:\n",
    "                group.create_dataset(name, shape=data.shape, dtype=data.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb0945be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ChannelMeta:\n",
    "    data_name: str\n",
    "    channel_label: str  # e.g. 'ch000'\n",
    "    figure_dirs: dict[str, Path]  # 'spike_detect', 'pca', 'sorting_cluster', 'auto_correlo'\n",
    "    h5_path: Path\n",
    "    log_path: Path\n",
    "    sampling_rate: int\n",
    "\n",
    "\n",
    "def process_channel(raw_wave: np.ndarray, cfg) -> dict:\n",
    "    \"\"\"\n",
    "    1 チャンネル分の波形からクラスタリング結果までをまとめて計算。\n",
    "    戻り値は可視化・保存で使う情報を dict で返す。\n",
    "    \"\"\"\n",
    "    filtered = BandPassFilter(\n",
    "        raw_wave,\n",
    "        bottom=cfg.band_bottom,\n",
    "        top=cfg.band_top,\n",
    "        sampling_rate=cfg.fs,\n",
    "    )\n",
    "    spike_idx = SpikeDetection(\n",
    "        filtered,\n",
    "        sd_thr=cfg.spike_threshold_sd,\n",
    "        order=cfg.spike_order,\n",
    "        spike=cfg.spike_polarity,\n",
    "    )\n",
    "\n",
    "    waveforms, spike_idx = GetWaveShape(\n",
    "        spike_idx,\n",
    "        filtered,\n",
    "        area_before_peak_ms=cfg.window_before_ms,\n",
    "        area_after_peak_ms=cfg.window_after_ms,\n",
    "        sampling_rate=cfg.fs,\n",
    "        ms=MS,\n",
    "    )\n",
    "    if spike_idx.size == 0:\n",
    "        return {\n",
    "            \"filtered\": filtered,\n",
    "            \"spike_idx\": spike_idx,\n",
    "            \"waveforms\": np.empty((0, 0)),\n",
    "            \"waveforms_roi\": np.empty((0, 0)),\n",
    "            \"features\": np.empty((0, cfg.pca_components)),\n",
    "            \"variance\": np.zeros(cfg.pca_components),\n",
    "            \"labels\": np.array([], dtype=int),\n",
    "            \"spike_times_ms\": np.array([]),\n",
    "        }\n",
    "\n",
    "    waveforms_roi = CutWaveShape(waveforms, area=cfg.cut_area)\n",
    "    x_pca, variance = DimensionalityReductionWithDiffs(waveforms_roi, cfg.pca_components)\n",
    "    features = StandardScaler().fit_transform(x_pca)\n",
    "\n",
    "    clusters = ClusteringWithHDBSCAN(features, clu_size=cfg.cluster_min_size, min_sam=cfg.cluster_min_samples)\n",
    "    merged = MargeCluster_TM(cluster=clusters, wave_shape=waveforms, thr_marge=cfg.template_merge_score)\n",
    "    refined = RescueNoise(\n",
    "        cluster=merged,\n",
    "        wave_shape=CutWaveShape(waveforms, area=cfg.cut_area),\n",
    "        thr_noise=cfg.noise_reassign_score,\n",
    "    )\n",
    "\n",
    "    spike_times_ms = spike_idx / (cfg.fs / MS)\n",
    "    isi = np.c_[np.arange(1, spike_times_ms.size + 1), spike_times_ms, np.diff(np.r_[0, spike_times_ms]), refined]\n",
    "\n",
    "    return {\n",
    "        \"filtered\": filtered,\n",
    "        \"spike_idx\": spike_idx,\n",
    "        \"waveforms\": waveforms,\n",
    "        \"waveforms_roi\": waveforms_roi,\n",
    "        \"features\": features,\n",
    "        \"variance\": variance,\n",
    "        \"labels\": refined,\n",
    "        \"spike_times_ms\": spike_times_ms,\n",
    "        \"isi\": isi,\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_and_save(meta: ChannelMeta, result: dict) -> None:\n",
    "    \"\"\"\n",
    "    Receive process_channel output, produce plots, and persist results in HDF5.\n",
    "    \"\"\"\n",
    "    filtered = result[\"filtered\"]\n",
    "    spike_idx = result[\"spike_idx\"]\n",
    "    waveforms = result[\"waveforms\"]\n",
    "    waveforms_roi = result[\"waveforms_roi\"]\n",
    "    features = result[\"features\"]\n",
    "    labels = result[\"labels\"]\n",
    "    spike_times_ms = result[\"spike_times_ms\"]\n",
    "    variance = result[\"variance\"]\n",
    "\n",
    "    # 検出波形\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    t = np.arange(filtered.size) / meta.sampling_rate\n",
    "    plt.plot(t, filtered, lw=0.5, color=\"steelblue\")\n",
    "    if spike_idx.size:\n",
    "        plt.plot(spike_idx / meta.sampling_rate, filtered[spike_idx], \"r.\", ms=3)\n",
    "    plt.xlabel(\"Time [s]\")\n",
    "    plt.ylabel(\"Filtered (uV)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(meta.figure_dirs[\"spike_detect\"] / f\"{meta.data_name}_{meta.channel_label}_spike_detect.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # PCA 図\n",
    "    if features.size:\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.scatter(features[:, 0], features[:, 1], s=5, c=\"gray\", alpha=0.5)\n",
    "        plt.xlabel(\"PC1\")\n",
    "        plt.ylabel(\"PC2\")\n",
    "        plt.title(f\"PCA scatter {meta.data_name}_{meta.channel_label}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"pca\"] / f\"{meta.data_name}_{meta.channel_label}_pca_raw.png\")\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        for clu, color in zip(np.unique(labels), itertools.cycle(COLOR)):\n",
    "            mask = labels == clu\n",
    "            plt.scatter(features[mask, 0], features[mask, 1], s=8, alpha=0.7, label=f\"clu {clu}\", c=color)\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.xlabel(\"PC1\")\n",
    "        plt.ylabel(\"PC2\")\n",
    "        plt.title(f\"PCA clustered {meta.data_name}_{meta.channel_label}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"pca\"] / f\"{meta.data_name}_{meta.channel_label}_pca_cluster.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # クラスタ波形\n",
    "    for clu, color in zip(np.unique(labels), itertools.cycle(COLOR)):\n",
    "        mask = labels == clu\n",
    "        if clu < 0 or not mask.any():\n",
    "            continue\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.plot(waveforms[mask].T, color=color, alpha=0.2, lw=0.5)\n",
    "        plt.plot(np.median(waveforms[mask], axis=0), color=color, lw=2)\n",
    "        plt.title(f\"{meta.channel_label} cluster {clu} (n={mask.sum()})\")\n",
    "        plt.xlabel(\"Samples\")\n",
    "        plt.ylabel(\"uV\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"sorting_cluster\"] / f\"{meta.data_name}_{meta.channel_label}_cluster{clu}_waveforms.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # ACR とパワースペクトラム\n",
    "    x_axis = np.arange(-1000, 1001)\n",
    "    for clu in np.unique(labels):\n",
    "        mask = labels == clu\n",
    "        if clu < 0 or not mask.any():\n",
    "            continue\n",
    "        acr = CalcACR(spike_times_ms[mask])\n",
    "\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.plot(x_axis, acr, color=\"black\", lw=1)\n",
    "        plt.xlim(-200, 200)\n",
    "        plt.xlabel(\"Time lag [ms]\")\n",
    "        plt.ylabel(\"Autocorrelation\")\n",
    "        plt.title(f\"{meta.channel_label} cluster {clu} autocorrelogram\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"auto_correlo\"] / f\"{meta.data_name}_{meta.channel_label}_cluster{clu}_acr.png\")\n",
    "        plt.close()\n",
    "\n",
    "        freqP = CalcPOW(acr, ex_file_path=\"\")\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.plot(freqP[:, 0], freqP[:, 1], color=\"black\", lw=1)\n",
    "        plt.xlim(0, 80)\n",
    "        plt.xlabel(\"Frequency [Hz]\")\n",
    "        plt.ylabel(\"Power/frequency\")\n",
    "        plt.title(f\"{meta.channel_label} cluster {clu} power spectrum\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(meta.figure_dirs[\"auto_correlo\"] / f\"{meta.data_name}_{meta.channel_label}_cluster{clu}_power.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # HDF5 保存\n",
    "    datasets = {\n",
    "        \"labels\": labels,\n",
    "        \"spike_times_ms\": spike_times_ms,\n",
    "        \"waveforms\": waveforms,\n",
    "        \"waveforms_roi\": waveforms_roi,\n",
    "        \"variance\": variance,\n",
    "    }\n",
    "    with h5py.File(meta.h5_path, \"a\") as h5:\n",
    "        group = h5.require_group(meta.channel_label)\n",
    "        for name, data in datasets.items():\n",
    "            if name in group:\n",
    "                del group[name]\n",
    "            if data.size:\n",
    "                group.create_dataset(name, data=data, compression=\"gzip\", compression_opts=4)\n",
    "            else:\n",
    "                group.create_dataset(name, shape=data.shape, dtype=data.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2507e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class SortingConfig:\n",
    "    fs: int = 20_000\n",
    "    band_bottom: int = 300\n",
    "    band_top: int = 3_000\n",
    "    band_numtaps: int = 255\n",
    "    spike_threshold_sd: float = 4.0\n",
    "    spike_order: int = 15\n",
    "    spike_polarity: int = -1\n",
    "    window_before_ms: float = 1.0\n",
    "    window_after_ms: float = 2.0\n",
    "    cut_area: int = 13\n",
    "    pca_components: int = 2\n",
    "    cluster_min_size: int = 2000\n",
    "    cluster_min_samples: int = 250\n",
    "    template_merge_score: int = 115\n",
    "    noise_reassign_score: int = 72\n",
    "    ch_array: np.ndarray = field(\n",
    "        default_factory=lambda: np.array([\n",
    "            'trigger', 'ch21', 'ch31', 'ch41', 'ch51', 'ch61', 'ch71', 'ch12',\n",
    "            'ch22', 'ch32', 'ch42', 'ch52', 'ch62', 'ch72', 'ch82', 'ch13', 'ch23',\n",
    "            'ch33', 'ch43', 'ch53', 'ch63', 'ch73', 'ch83', 'ch14', 'ch24', 'ch34',\n",
    "            'ch44', 'ch54', 'ch64', 'ch74', 'ch84', 'ch15', 'ch25', 'ch35', 'ch45',\n",
    "            'ch55', 'ch65', 'ch75', 'ch85', 'ch16', 'ch26', 'ch36', 'ch46', 'ch56',\n",
    "            'ch66', 'ch76', 'ch86', 'ch17', 'ch27', 'ch37', 'ch47', 'ch57', 'ch67',\n",
    "            'ch77', 'ch87', 'ch28', 'ch38', 'ch48', 'ch58', 'ch68', 'ch78'\n",
    "        ])\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "248715c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_path = Path(raw_path)              # 文字列なら Path 化\n",
    "# data_name = raw_path.stem             # 例: '01_Flash_0001'\n",
    "# base_dir = Path(output_root)\n",
    "# dirs = setup_output_dirs(base_dir)    # 例として spike_detect / pca / sorting_cluster / auto_correlo / npy を返す関数\n",
    "# figure_dirs = {\n",
    "#     \"spike_detect\": dirs[\"spike_detect\"],\n",
    "#     \"pca\": dirs[\"pca\"],\n",
    "#     \"sorting_cluster\": dirs[\"sorting_cluster\"],\n",
    "#     \"auto_correlo\": dirs[\"auto_correlo\"],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eda463f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_raw_matrix は後続セルでチャネル設定と合わせて実行します\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df18cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"./temp_results\")  # 出力先\n",
    "base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "figure_dirs = {\n",
    "    \"spike_detect\": base_dir / \"spike_detect\",\n",
    "    \"pca\": base_dir / \"pca\",\n",
    "    \"sorting_cluster\": base_dir / \"sorting_cluster\",\n",
    "    \"auto_correlo\": base_dir / \"auto_correlo\",\n",
    "}\n",
    "for path in figure_dirs.values():\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "h5_path = base_dir / f\"{data_name}_spike_sort.h5\"\n",
    "if h5_path.exists():\n",
    "    h5_path.unlink()\n",
    "\n",
    "log_path = base_dir / f\"{data_name}_spike_sort.log\"\n",
    "with log_path.open(\"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(f\"Spike sorting log for {data_name} (created {datetime.now().isoformat()})\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecbb64ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 6 positional arguments but 7 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m wave \u001b[38;5;241m=\u001b[39m extract_channel(raw_matrix, ch_label, ch_array\u001b[38;5;241m=\u001b[39mchannel_array)\n\u001b[0;32m      7\u001b[0m result \u001b[38;5;241m=\u001b[39m process_channel(wave, cfg)\n\u001b[1;32m----> 8\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[43mChannelMeta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mch_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigure_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m visualize_and_save(meta, result)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 6 positional arguments but 7 were given"
     ]
    }
   ],
   "source": [
    "channel_array = np.array(channels, dtype=str)\n",
    "cfg = SortingConfig(ch_array=channel_array)\n",
    "raw_matrix = load_raw_matrix(raw_path, channel_num)\n",
    "\n",
    "for ch_label in channel_array[0:1]:\n",
    "    wave = extract_channel(raw_matrix, ch_label, ch_array=channel_array)\n",
    "    result = process_channel(wave, cfg)\n",
    "    meta = ChannelMeta(data_name, ch_label, figure_dirs, h5_path, log_path, cfg.fs)\n",
    "    visualize_and_save(meta, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d20bc1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_name', 'channel_label', 'figure_dirs', 'h5_path', 'sampling_rate']\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import fields\n",
    "\n",
    "print([f.name for f in fields(ChannelMeta)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ad80b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 6 positional arguments but 7 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[56], line 8\u001b[0m\n",
      "\u001b[0;32m      6\u001b[0m wave \u001b[38;5;241m=\u001b[39m extract_channel(raw_matrix, ch_label, ch_array\u001b[38;5;241m=\u001b[39mchannel_array)\n",
      "\u001b[0;32m      7\u001b[0m result \u001b[38;5;241m=\u001b[39m process_channel(wave, cfg)\n",
      "\u001b[1;32m----> 8\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[43mChannelMeta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mch_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigure_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      9\u001b[0m visualize_and_save(meta, result)\n",
      "\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 6 positional arguments but 7 were given"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9066c27a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 6 positional arguments but 7 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[56], line 8\u001b[0m\n",
      "\u001b[0;32m      6\u001b[0m wave \u001b[38;5;241m=\u001b[39m extract_channel(raw_matrix, ch_label, ch_array\u001b[38;5;241m=\u001b[39mchannel_array)\n",
      "\u001b[0;32m      7\u001b[0m result \u001b[38;5;241m=\u001b[39m process_channel(wave, cfg)\n",
      "\u001b[1;32m----> 8\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[43mChannelMeta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mch_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigure_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      9\u001b[0m visualize_and_save(meta, result)\n",
      "\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 6 positional arguments but 7 were given"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332b85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Imaris\\Desktop\\watanabe\\250801\\リポジトリ\n",
      "test_ch000_cluster0_spikes.npy (10819,) [2069.65 9101.35 9116.8  9135.45 9138.8  9148.55 9206.75 9233.8  9279.6\n",
      " 9305.4 ]\n",
      "test_ch000_cluster1_spikes.npy (4928,) [ 963.05 2614.7  3001.25 3925.8  4004.15 4131.2  4537.05 4705.6  5235.7\n",
      " 5284.9 ]\n",
      "test_ch000_cluster2_spikes.npy (45117,) [ 60.25 200.25 268.15 385.9  439.   512.1  592.85 796.45 798.75 812.7 ]\n",
      "test_ch000_cluster_labels.npy (151881,) [-1 -1  2 -1 -1 -1  2 -1  2 -1]\n",
      "test_ch000_spike_times_ms.npy (151881,) [ 15.85  58.35  60.25  87.85 134.75 145.15 200.25 258.8  268.15 296.25]\n",
      "test_ch000_variance.npy (2,) [0.55240433 0.09035588]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "print(os.getcwd())  # まずはカレントディレクトリ確認\n",
    "\n",
    "base = Path(r\"C:/Users/Imaris/Desktop/watanabe/250801/リポジトリ/spike_sort_test_20250919_155648/npy\")\n",
    "for path in base.glob(\"*.npy\"):\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    print(path.name, data.shape, data[:10] if data.ndim == 1 else data[:10, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e9d207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294fcaac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mea2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}